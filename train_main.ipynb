{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1b7ecdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from utils import *\n",
    "from utils_data import *\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "from models import Glow\n",
    "from models.glow.coupling import UNet1\n",
    "import util\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as sched\n",
    "import torch.backends.cudnn as cudnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9069b686",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--mode', type=str, default='ct')\n",
    "parser.add_argument('--noise_level', type=list, default=[5000]) # For PET-CT, noise_level = [PET, CT]\n",
    "parser.add_argument('--semi_sup', type=bool, default=True)\n",
    "parser.add_argument('--supervision', type=float, default=0.0)\n",
    "parser.add_argument('--secondary_noisy', type=int, default=0)\n",
    "parser.add_argument('--resume_training', type=int, default=0)\n",
    "parser.add_argument('--train_size', type=int, default=200)\n",
    "parser.add_argument('--blur_mode',type=str, default=None)\n",
    "parser.add_argument('--new_range',type=int, default=2)\n",
    "\n",
    "parser.add_argument('--transfer_learning', type=bool, default=False)\n",
    "parser.add_argument('--transfer_path', type=str, default='../results200_nd/unet_var_multi5/e3sgdws_petct_bpetnoperc_unet_var_ggg_multif_semi_0.0005-5000_0.5/model_400.ckpt')\n",
    "\n",
    "parser.add_argument('--n_samples', type=int, default=1)\n",
    "parser.add_argument('--s_samples', type=int, default=1)\n",
    "parser.add_argument('--batch_size', type=int, default=1)\n",
    "parser.add_argument('--epoch_num', type=int, default=3000)\n",
    "parser.add_argument('--lr', type=float, default=1e-2)\n",
    "parser.add_argument('--device', type=str, default='cuda:1')\n",
    "parser.add_argument('--perceptual', type=bool, default=False)\n",
    "parser.add_argument('--weights',type=tuple,default=[1, 1, 1]) #(pet, ct, latent)\n",
    "parser.add_argument('--target',type=str,default=None)\n",
    "\n",
    "parser.add_argument('--save', type=bool, default=True)\n",
    "parser.add_argument('--path', type=str, default='../results/e2sgd_')\n",
    "parser.add_argument('--save_path', type=str, default='')\n",
    "parser.add_argument('--save_path_fig', type=str, default='')\n",
    "\n",
    "def str2bool(s):\n",
    "    return s.lower().startswith('t')\n",
    "parser.add_argument('--num_levels', '-L', default=4, type=int, help='Number of levels in the Glow model')\n",
    "parser.add_argument('--num_steps', '-K', default=8, type=int, help='Number of steps of flow in each level')\n",
    "parser.add_argument('--cc', type = str2bool, default = False)\n",
    "parser.add_argument('--warm_up', default=500000, type=int, help='Number of steps for lr warm-up')\n",
    "parser.add_argument('--ext', default = 'll', type=str)\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "# args_check(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04306c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 1, 512, 512) (326, 1, 512, 512)\n",
      "same used\n",
      "supervised\n",
      "supervised\n"
     ]
    }
   ],
   "source": [
    "trainloader, testloader, validloader = load_data(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8bafbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Glow(num_channels=1,\n",
    "               num_levels=args.num_levels,\n",
    "               num_steps=args.num_steps,\n",
    "               inp_channel=1,\n",
    "               cond_channel=1,\n",
    "               cc = args.cc)\n",
    "net = net.to(args.device)\n",
    "# cudnn.benchmark = True\n",
    "# if device == 'cuda':\n",
    "#     net = torch.nn.DataParallel(net, args.gpu_ids)\n",
    "#     cudnn.benchmark = args.benchmark\n",
    "\n",
    "unet = UNet1(inp_channels=1, op_channels=1)\n",
    "unet = unet.to(args.device)\n",
    "# unet_weights = torch.load('ckpts/unet/best.pth', map_location = args.device)\n",
    "# unet.load_state_dict(unet_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7194129a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.enable_grad()\n",
    "def train(epoch, net, trainloader, device, optimizer, scheduler, loss_fn, max_grad_norm = -1, type = 'ct', args = None, model = None, epsilon = 1e-1):\n",
    "    global global_step\n",
    "    global_step = 0\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    latent_loss_m = util.AverageMeter()\n",
    "    spatial_loss_m = util.AverageMeter()\n",
    "    idx1, idx2 = get_idx(type)\n",
    "    smooth_l1_loss = torch.nn.SmoothL1Loss().to(device)\n",
    "\n",
    "    with tqdm(total=len(trainloader.dataset)) as progress_bar:\n",
    "        for i, x_prime in enumerate(trainloader):\n",
    "            x = x_prime[1] #x_prime[:, idx1, :, :]\n",
    "            cond_x = x_prime[0] #x_prime[:, idx2, :, :]\n",
    "#             mask = x_prime[:, 4, :, :].unsqueeze(1).to(device)\n",
    "#             mask = torch.where(mask > 0, 1, 0)\n",
    "            if len(x.shape) < 4:\n",
    "                x = x.unsqueeze(1)\n",
    "            if len(cond_x.shape) < 4:\n",
    "                cond_x = cond_x.unsqueeze(1)\n",
    "            x, cond_x = x.to(device), cond_x.to(device)\n",
    "            cond_x.requires_grad = True\n",
    "            z, sldj = net(x, cond_x, reverse=False)\n",
    "\n",
    "            if args.ext == 'll+sl_pl' or args.ext == 'll+sl_l1':\n",
    "                sl_name = args.ext.split('_')[1]\n",
    "                optimizer.zero_grad()\n",
    "                latent_loss = loss_fn(z, sldj)\n",
    "                new_z = torch.randn(x.shape, dtype=torch.float32, device=device) * 0.6\n",
    "                rec_x, sldj = net(new_z, cond_x, reverse=True)\n",
    "                rec_x = torch.sigmoid(rec_x) #mask * torch.sigmoid(rec_x)\n",
    "                if sl_name == 'pl':\n",
    "                    spatial_loss = perceptual_loss(rec_x, x, model, smooth_l1_loss)\n",
    "                elif sl_name == 'l1':\n",
    "                    spatial_loss = smooth_l1_loss(rec_x, x)\n",
    "\n",
    "                loss = latent_loss + spatial_loss\n",
    "                loss.backward()\n",
    "                if max_grad_norm > 0:\n",
    "                    util.clip_grad_norm(optimizer, max_grad_norm)\n",
    "                optimizer.step()\n",
    "                scheduler.step(global_step)\n",
    "\n",
    "                latent_loss_m.update(latent_loss.item(), x.size(0))\n",
    "                spatial_loss_m.update(spatial_loss.item(), x.size(0))\n",
    "                progress_bar.set_postfix(bpd=util.bits_per_dim(x, latent_loss_m.avg),\n",
    "                                         pl = spatial_loss_m.avg)\n",
    "                progress_bar.update(x.size(0))\n",
    "                global_step += x.size(0)\n",
    "\n",
    "                # Adversarial Examples Training\n",
    "\n",
    "                # Calculating FGSM\n",
    "                cond_x = cond_x + (epsilon * torch.sign(cond_x.grad))\n",
    "                cond_x = torch.clamp(cond_x, 0, 1)\n",
    "\n",
    "                # Feeding to the network and calculating loss\n",
    "                z, sldj = net(x, cond_x, reverse=False)\n",
    "                optimizer.zero_grad()\n",
    "                latent_loss = loss_fn(z, sldj)\n",
    "                new_z = torch.randn(x.shape, dtype=torch.float32, device=device) * 0.6\n",
    "                rec_x, sldj = net(new_z, cond_x, reverse=True)\n",
    "                rec_x = torch.sigmoid(rec_x) #mask * torch.sigmoid(rec_x)\n",
    "                if sl_name == 'pl':\n",
    "                    spatial_loss = perceptual_loss(rec_x, x, model, smooth_l1_loss)\n",
    "                elif sl_name == 'l1':\n",
    "                    spatial_loss = smooth_l1_loss(rec_x, x)\n",
    "                loss = latent_loss + spatial_loss\n",
    "\n",
    "                # Backpropagation\n",
    "                loss.backward()\n",
    "                if max_grad_norm > 0:\n",
    "                    util.clip_grad_norm(optimizer, max_grad_norm)\n",
    "                optimizer.step()\n",
    "                scheduler.step(global_step)\n",
    "\n",
    "                # Updating the progress bar\n",
    "                latent_loss_m.update(latent_loss.item(), x.size(0))\n",
    "                spatial_loss_m.update(spatial_loss.item(), x.size(0))\n",
    "                progress_bar.set_postfix(bpd=util.bits_per_dim(x, latent_loss_m.avg),\n",
    "                                         pl = spatial_loss_m.avg)\n",
    "                progress_bar.update(x.size(0))\n",
    "                global_step += x.size(0)\n",
    "\n",
    "            # elif args.ext == 'll_then_sl':\n",
    "            #     optimizer.zero_grad()\n",
    "            #     latent_loss = loss_fn(z, sldj)\n",
    "            #     latent_loss.backward()\n",
    "            #     if max_grad_norm > 0:\n",
    "            #         util.clip_grad_norm(optimizer, max_grad_norm)\n",
    "            #     optimizer.step()\n",
    "            #     scheduler.step(global_step)\n",
    "\n",
    "            #     new_z = torch.randn(x.shape, dtype=torch.float32, device=device) * 0.6\n",
    "            #     rec_x, sldj = net(new_z, cond_x, reverse=True)\n",
    "            #     rec_x = mask * torch.sigmoid(rec_x)\n",
    "            #     optimizer.zero_grad()\n",
    "            #     # l1_loss = smooth_l1_loss(rec_x, x)\n",
    "            #     # ssim_loss = 1 - ssim1(rec_x, x, data_range = 1)\n",
    "            #     # spatial_loss = l1_loss + ssim_loss\n",
    "            #     spatial_loss = perceptual_loss(rec_x, x, model, smooth_l1_loss)\n",
    "            #     spatial_loss.backward()\n",
    "            #     if max_grad_norm > 0:\n",
    "            #         util.clip_grad_norm(optimizer, max_grad_norm)\n",
    "            #     optimizer.step()\n",
    "            #     scheduler.step(global_step)\n",
    "            #     print(f'll:{latent_loss}, sl:{spatial_loss}')\n",
    "            #     latent_loss_m.update(latent_loss.item(), x.size(0))\n",
    "            #     # l1_loss_m.update(l1_loss.item(), x.size(0))\n",
    "            #     # ssim_loss_m.update(ssim_loss.item(), x.size(0))\n",
    "            #     perceptual_loss_m.update(spatial_loss.item(), x.size(0))\n",
    "            #     progress_bar.set_postfix(bpd=util.bits_per_dim(x, latent_loss_m.avg),\n",
    "            #                              pl=perceptual_loss_m.avg)\n",
    "            #                             # ssim=ssim_loss_m.avg,\n",
    "            #                             # l1=l1_loss_m.avg)\n",
    "            #     progress_bar.update(x.size(0))\n",
    "            #     global_step += x.size(0)\n",
    "\n",
    "            elif args.ext == 'll':\n",
    "                optimizer.zero_grad()\n",
    "                latent_loss = loss_fn(z, sldj)\n",
    "                latent_loss.backward()\n",
    "                if max_grad_norm > 0:\n",
    "                    util.clip_grad_norm(optimizer, max_grad_norm)\n",
    "                optimizer.step()\n",
    "                scheduler.step(global_step)\n",
    "\n",
    "                latent_loss_m.update(latent_loss.item(), x.size(0))\n",
    "                progress_bar.set_postfix(nll=latent_loss_m.avg,\n",
    "                                        bpd=util.bits_per_dim(x, latent_loss_m.avg),\n",
    "                                        lr=optimizer.param_groups[0]['lr'])\n",
    "                progress_bar.update(x.size(0))\n",
    "                global_step += x.size(0)\n",
    "\n",
    "                # Adversarial Examples Training\n",
    "                # fig, ax = plt.subplots(1, 3, figsize=(30, 30))\n",
    "                # ax[0].imshow(cond_x[0,0,:,:].detach().cpu(), cmap='gray')\n",
    "                # im = ax[1].imshow((epsilon * cond_x.grad)[0,0,:,:].detach().cpu(), cmap='gray')\n",
    "                cond_x = cond_x + (epsilon * torch.sign(cond_x.grad))\n",
    "                # ax[2].imshow(cond_x[0,0,:,:].detach().cpu(), cmap='gray')\n",
    "                # ax[0].axis('off')\n",
    "                # ax[1].axis('off')\n",
    "                # ax[2].axis('off')\n",
    "                # cbar = plt.colorbar(im, ax=ax, orientation = 'horizontal', \n",
    "                #  pad = 0.01, aspect = 100)\n",
    "                # plt.savefig(f'test.png', bbox_inches='tight')\n",
    "                cond_x = torch.clamp(cond_x, 0, 1)\n",
    "                z, sldj = net(x, cond_x, reverse=False)\n",
    "                optimizer.zero_grad()\n",
    "                latent_loss = loss_fn(z, sldj)\n",
    "                latent_loss.backward()\n",
    "                if max_grad_norm > 0:\n",
    "                    util.clip_grad_norm(optimizer, max_grad_norm)\n",
    "                optimizer.step()\n",
    "                scheduler.step(global_step)\n",
    "                \n",
    "        \n",
    "                latent_loss_m.update(latent_loss.item(), x.size(0))\n",
    "                progress_bar.set_postfix(nll=latent_loss_m.avg,\n",
    "                                        bpd=util.bits_per_dim(x, latent_loss_m.avg),\n",
    "                                        lr=optimizer.param_groups[0]['lr'])\n",
    "                progress_bar.update(x.size(0))\n",
    "                global_step += x.size(0)\n",
    "\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(epoch, net, testloader, device, args, path, history = []):\n",
    "    global best_ssim\n",
    "    global best_epoch\n",
    "    net.eval()\n",
    "\n",
    "    rrmse_val, psnr_val, ssim_val = evaluate_1c(net, testloader, device, args.type)\n",
    "    ssim = np.mean(ssim_val)\n",
    "    flag = True\n",
    "    history.append(ssim)\n",
    "\n",
    "    # Save checkpoint\n",
    "    if torch.isnan(torch.tensor(ssim)):\n",
    "        return True\n",
    "\n",
    "    if flag and ssim > best_ssim:\n",
    "        print('Saving...')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'ssim': ssim,\n",
    "            'rrmse': np.mean(rrmse_val),\n",
    "            'psnr': np.mean(psnr_val),\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        path1 = '/'.join(path.split('/')[:-1])\n",
    "        os.makedirs(path1, exist_ok=True)\n",
    "        torch.save(state, path)        \n",
    "        best_ssim = ssim\n",
    "        best_epoch = epoch\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2d28528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]/home/vatsala/miniconda3/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "  4%|â–Ž         | 7/200 [00:09<04:08,  1.29s/it, bpd=nan, lr=1.2e-7, nll=nan]   \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "svd_cuda: For batch 0: U(257,257) is zero, singular U.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3376734/2332007047.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_num\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     train(epoch, net, trainloader, args.device, optimizer, scheduler,\n\u001b[0m\u001b[1;32m     13\u001b[0m           loss_fn, type = args.mode, args = args, model=unet)\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3376734/182605488.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, net, trainloader, device, optimizer, scheduler, loss_fn, max_grad_norm, type, args, model, epsilon)\u001b[0m\n\u001b[1;32m    150\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m                 \u001b[0mlatent_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msldj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m                 \u001b[0mlatent_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmax_grad_norm\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m                     \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mRuntimeError\u001b[0m: svd_cuda: For batch 0: U(257,257) is zero, singular U."
     ]
    }
   ],
   "source": [
    "loss_fn = util.NLLLoss().to(args.device)\n",
    "# loss_fn = util.NLLLoss(shape = args.shape, device = device).to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=args.lr)\n",
    "scheduler = sched.LambdaLR(optimizer, lambda s: min(1., s / args.warm_up))\n",
    "start_epoch = 0\n",
    "\n",
    "epoch = start_epoch\n",
    "c = 0\n",
    "history = []\n",
    "while epoch <= args.epoch_num:\n",
    "    c += 1\n",
    "    train(epoch, net, trainloader, args.device, optimizer, scheduler,\n",
    "          loss_fn, type = args.mode, args = args, model=unet)\n",
    "    if test(epoch, net, testloader, device, args, path, history):\n",
    "        if os.path.exists(path):  \n",
    "            checkpoint = torch.load(path, map_location = device)\n",
    "            net.load_state_dict(checkpoint['net'])\n",
    "            best_ssim = checkpoint['ssim']\n",
    "            best_epoch = checkpoint['epoch']\n",
    "            epoch = best_epoch\n",
    "            print('Loaded previous model...')\n",
    "        else:\n",
    "            net = Glow(num_channels=args.num_channels,\n",
    "                        num_levels=args.num_levels,\n",
    "                        num_steps=args.num_steps,\n",
    "                        mode=args.mode,\n",
    "                        inp_channel=args.inp_channel,\n",
    "                        cond_channel=args.cond_channel,\n",
    "                        cc = args.cc)\n",
    "            net = net.to(device)\n",
    "            if device == 'cuda':\n",
    "                net = torch.nn.DataParallel(net, args.gpu_ids)\n",
    "                cudnn.benchmark = args.benchmark\n",
    "            best_ssim = 0\n",
    "            best_epoch = start_epoch\n",
    "            epoch = start_epoch\n",
    "            print('Initialized new model!')\n",
    "\n",
    "        optimizer = optim.Adam(net.parameters(), lr=args.lr)\n",
    "        scheduler = sched.LambdaLR(optimizer, lambda s: min(1., s / args.warm_up))\n",
    "    else:\n",
    "        epoch += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2385d38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755daca5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
